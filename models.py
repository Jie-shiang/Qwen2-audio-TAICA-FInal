# -*- coding: utf-8 -*-
"""
models.py - æ¨¡å‹ç®¡ç†ä¸­å¿ƒ
è² è²¬æ‰€æœ‰AIæ¨¡å‹çš„è¼‰å…¥ã€é…ç½®å’Œç®¡ç†
"""

import torch
import whisper
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
import gc
import warnings
from memory_monitor import start_memory_monitoring, get_memory_monitor
warnings.filterwarnings("ignore")

class ModelManager:    
    def __init__(self, gpu_memory_limit=20):
        self.gpu_memory_limit = gpu_memory_limit
        self.device = None
        self.use_gpu = False
        self.whisper_model = None
        self.audio_llm_model = None
        self.audio_llm_processor = None
        self.use_audio_llm = False
        self.memory_monitor = None
        
        # åˆå§‹åŒ–
        self._setup_gpu()
        self._start_memory_monitoring()
        self._load_models()
    
    def _start_memory_monitoring(self):
        print(f"ğŸ” å•Ÿå‹•è¨˜æ†¶é«”ç›£æ§ (é™åˆ¶: {self.gpu_memory_limit}GB)")
        self.memory_monitor = start_memory_monitoring(
            gpu_limit_gb=self.gpu_memory_limit,
            cpu_limit_gb=32,
            check_interval=3
        )
    
    def _setup_gpu(self):
        print("=== GPUè¨­å®šæª¢æŸ¥ ===")
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            print(f"å¯ç”¨GPUæ•¸é‡: {gpu_count}")
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
            
            self.device = torch.device("cuda:0")
            torch.cuda.set_device(0)
            self.use_gpu = True
            print(f"ä½¿ç”¨è¨­å‚™: {self.device}")
            
            torch.cuda.empty_cache()
            gc.collect()
        else:
            print("CUDAä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨CPU")
            self.device = torch.device("cpu")
            self.use_gpu = False
    
    def _memory_check_and_cleanup(self, operation_name=""):
        if not self.use_gpu:
            return True
            
        try:
            current_memory = torch.cuda.memory_reserved(0) / 1024**3
            if current_memory > self.gpu_memory_limit * 0.9:  # 90%è­¦å‘Š
                print(f"âš ï¸  {operation_name} - è¨˜æ†¶é«”ä½¿ç”¨æ¥è¿‘é™åˆ¶: {current_memory:.2f}GB")
                self.clear_gpu_memory()
                
                # å†æ¬¡æª¢æŸ¥
                current_memory = torch.cuda.memory_reserved(0) / 1024**3
                if current_memory > self.gpu_memory_limit:
                    print(f"ğŸš¨ è¨˜æ†¶é«”ä»è¶…éé™åˆ¶: {current_memory:.2f}GB > {self.gpu_memory_limit}GB")
                    return False
            
            return True
        except Exception as e:
            print(f"è¨˜æ†¶é«”æª¢æŸ¥å¤±æ•—: {e}")
            return True
    
    def _load_whisper_model(self):
        print("æ­£åœ¨è¼‰å…¥Whisperæ¨¡å‹...")
        
        if not self._memory_check_and_cleanup("Whisperè¼‰å…¥å‰"):
            print("è¨˜æ†¶é«”ä¸è¶³ï¼Œç„¡æ³•è¼‰å…¥Whisperæ¨¡å‹")
            return False
            
        try:
            if self.use_gpu:
                self.whisper_model = whisper.load_model("medium").to(self.device)
                print("Whisperæ¨¡å‹å·²è¼‰å…¥è‡³GPU")
                
                if not self._memory_check_and_cleanup("Whisperè¼‰å…¥å¾Œ"):
                    print("Whisperè¼‰å…¥å¾Œè¨˜æ†¶é«”è¶…é™ï¼Œé™ç´šä½¿ç”¨baseæ¨¡å‹")
                    del self.whisper_model
                    self.clear_gpu_memory()
                    self.whisper_model = whisper.load_model("base").to(self.device)
                    
            else:
                self.whisper_model = whisper.load_model("base")
                print("Whisperæ¨¡å‹å·²è¼‰å…¥è‡³CPU")
            return True
        except Exception as e:
            print(f"Whisperè¼‰å…¥å¤±æ•—: {e}")
            try:
                self.whisper_model = whisper.load_model("base")
                print("å·²è¼‰å…¥åŸºç¤ç‰ˆWhisperæ¨¡å‹")
                return True
            except Exception as e2:
                print(f"åŸºç¤ç‰ˆWhisperä¹Ÿè¼‰å…¥å¤±æ•—: {e2}")
                return False
    
    def _load_qwen_audio_model(self):
        print("æ­£åœ¨è¼‰å…¥Qwen2-Audioæ¨¡å‹...")
        
        if not self._memory_check_and_cleanup("Qwen2-Audioè¼‰å…¥å‰"):
            print("è¨˜æ†¶é«”ä¸è¶³ï¼Œè·³éQwen2-Audioæ¨¡å‹è¼‰å…¥")
            self.use_audio_llm = False
            return False
            
        try:
            if self.use_gpu:
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                current_memory = torch.cuda.memory_reserved(0) / 1024**3
                available_memory = gpu_memory - current_memory
                
                print(f"GPUè¨˜æ†¶é«”: ç¸½è¨ˆ{gpu_memory:.1f}GB, å·²ç”¨{current_memory:.1f}GB, å¯ç”¨{available_memory:.1f}GB")
                
                if available_memory < 6:
                    print("å¯ç”¨è¨˜æ†¶é«”ä¸è¶³6GBï¼Œä½¿ç”¨CPUæ¨¡å¼")
                    torch_dtype = torch.float32
                    device_map = "cpu"
                elif available_memory < 10:
                    print("å¯ç”¨è¨˜æ†¶é«”æœ‰é™ï¼Œä½¿ç”¨float16å’Œé‡åŒ–")
                    torch_dtype = torch.float16
                    device_map = {"": 0}
                else:
                    torch_dtype = torch.float16
                    device_map = "auto"
            else:
                torch_dtype = torch.float32
                device_map = "cpu"

            self.audio_llm_model = Qwen2AudioForConditionalGeneration.from_pretrained(
                "Qwen/Qwen2-Audio-7B-Instruct",
                torch_dtype=torch_dtype,
                device_map=device_map,
                trust_remote_code=True,
                low_cpu_mem_usage=True
            )

            self.audio_llm_model.tie_weights()
            
            if not self._memory_check_and_cleanup("Qwen2-Audioè¼‰å…¥å¾Œ"):
                print("âš ï¸  Qwen2-Audioè¼‰å…¥å¾Œè¨˜æ†¶é«”è¶…é™")
                del self.audio_llm_model
                self.clear_gpu_memory()
                self.use_audio_llm = False
                return False

            self.audio_llm_processor = AutoProcessor.from_pretrained(
                "Qwen/Qwen2-Audio-7B-Instruct",
                trust_remote_code=True
            )

            print("Qwen2-Audioæ¨¡å‹è¼‰å…¥å®Œæˆï¼")
            self.use_audio_llm = True
            return True

        except torch.cuda.OutOfMemoryError:
            print("GPUè¨˜æ†¶é«”ä¸è¶³ï¼Œå˜—è©¦ä½¿ç”¨CPU...")
            torch.cuda.empty_cache()
            gc.collect()
            try:
                self.audio_llm_model = Qwen2AudioForConditionalGeneration.from_pretrained(
                    "Qwen/Qwen2-Audio-7B-Instruct",
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                self.audio_llm_processor = AutoProcessor.from_pretrained(
                    "Qwen/Qwen2-Audio-7B-Instruct",
                    trust_remote_code=True
                )
                print("Qwen2-Audioæ¨¡å‹å·²è¼‰å…¥è‡³CPU")
                self.use_audio_llm = True
                return True
            except Exception as e:
                print(f"CPUè¼‰å…¥ä¹Ÿå¤±æ•—: {e}")
                self.use_audio_llm = False
                return False

        except Exception as e:
            print(f"Qwen2-Audioæ¨¡å‹è¼‰å…¥å¤±æ•—: {str(e)}")
            print("å°‡ä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬çš„ç™¼éŸ³åˆ†æåŠŸèƒ½")
            self.use_audio_llm = False
            return False
    
    def _load_models(self):
        """è¼‰å…¥æ‰€æœ‰æ¨¡å‹"""
        print("=== é–‹å§‹è¼‰å…¥æ¨¡å‹ ===")
        
        whisper_success = self._load_whisper_model()
        if not whisper_success:
            raise Exception("Whisperæ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œç„¡æ³•ç¹¼çºŒ")
        
        qwen_success = self._load_qwen_audio_model()
        
        self._memory_check_and_cleanup("æ‰€æœ‰æ¨¡å‹è¼‰å…¥å¾Œ")
        
        print("=== æ¨¡å‹è¼‰å…¥å®Œæˆ ===")
        print(f"Whisper: {'âœ“' if whisper_success else 'âœ—'}")
        print(f"Qwen2-Audio: {'âœ“' if qwen_success else 'âœ—'}")
        print(f"è¨˜æ†¶é«”ç›£æ§: {'âœ“' if self.memory_monitor else 'âœ—'}")
    
    def get_device_info(self):
        """ç²å–è¨­å‚™ä¿¡æ¯"""
        info = {
            "device": str(self.device),
            "use_gpu": self.use_gpu,
            "use_audio_llm": self.use_audio_llm,
            "whisper_available": self.whisper_model is not None,
            "qwen_available": self.audio_llm_model is not None,
            "memory_limit_gb": self.gpu_memory_limit
        }
        
        if self.use_gpu:
            info["gpu_memory"] = torch.cuda.get_device_properties(0).total_memory / 1024**3
            info["gpu_name"] = torch.cuda.get_device_name(0)
            info["current_memory_usage"] = torch.cuda.memory_reserved(0) / 1024**3
        
        return info
    
    def clear_gpu_memory(self):
        """æ¸…ç†GPUè¨˜æ†¶é«”"""
        if self.use_gpu:
            torch.cuda.empty_cache()
            gc.collect()
            print(f"ğŸ§¹ GPUè¨˜æ†¶é«”å·²æ¸…ç†ï¼Œç•¶å‰ä½¿ç”¨: {torch.cuda.memory_reserved(0) / 1024**3:.2f}GB")
    
    def transcribe_audio(self, audio_path, language="en"):
        """ä½¿ç”¨Whisperé€²è¡ŒèªéŸ³è­˜åˆ¥"""
        if self.whisper_model is None:
            raise Exception("Whisperæ¨¡å‹æœªè¼‰å…¥")
        
        if not self._memory_check_and_cleanup("èªéŸ³è­˜åˆ¥å‰"):
            raise Exception("è¨˜æ†¶é«”ä¸è¶³ï¼Œç„¡æ³•é€²è¡ŒèªéŸ³è­˜åˆ¥")
        
        try:
            if self.use_gpu:
                result = self.whisper_model.transcribe(
                    audio_path, 
                    language=language, 
                    temperature=0.0, 
                    verbose=False,
                    fp16=True
                )
            else:
                result = self.whisper_model.transcribe(
                    audio_path, 
                    language=language, 
                    temperature=0.0, 
                    verbose=False
                )
            
            self._memory_check_and_cleanup("èªéŸ³è­˜åˆ¥å¾Œ")
            
            return result["text"].strip()
        except Exception as e:
            print(f"èªéŸ³è­˜åˆ¥éŒ¯èª¤: {e}")
            return None
    
    def generate_audio_response(self, audio_path, prompt, max_tokens=256):
        """ä½¿ç”¨Qwen2-Audioç”Ÿæˆå›æ‡‰"""
        if not self.use_audio_llm:
            return None
        
        if not self._memory_check_and_cleanup("Audio-LLMç”Ÿæˆå‰"):
            print("è¨˜æ†¶é«”ä¸è¶³ï¼Œè·³éAudio-LLMç”Ÿæˆ")
            return None
        
        try:
            import librosa
            audio_data, sr = librosa.load(audio_path, sr=16000)
            
            max_length = 30 * sr
            if len(audio_data) > max_length:
                audio_data = audio_data[:max_length]
            
            # è™•ç†è¼¸å…¥
            with torch.no_grad():
                inputs = self.audio_llm_processor(
                    text=prompt,
                    audio=audio_data,
                    sampling_rate=sr,
                    return_tensors="pt",
                    padding=True
                )

                if self.use_gpu:
                    inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                             for k, v in inputs.items()}

                if not self._memory_check_and_cleanup("Audio-LLMç”Ÿæˆä¸­"):
                    return None

                generate_ids = self.audio_llm_model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    do_sample=True,
                    top_p=0.95,
                    pad_token_id=self.audio_llm_processor.tokenizer.eos_token_id
                )

                generated_ids = generate_ids[:, inputs['input_ids'].size(1):]
                response = self.audio_llm_processor.decode(generated_ids[0], skip_special_tokens=True)
                
                del inputs, generate_ids, generated_ids
                self.clear_gpu_memory()
                
                return response

        except torch.cuda.OutOfMemoryError:
            print("ğŸš¨ GPUè¨˜æ†¶é«”ä¸è¶³ï¼ŒAudio-LLMç”Ÿæˆå¤±æ•—")
            self.clear_gpu_memory()
            return None
        except Exception as e:
            print(f"Audio-LLMç”ŸæˆéŒ¯èª¤: {e}")
            self.clear_gpu_memory()
            return None
    
    def get_memory_status(self):
        if self.memory_monitor:
            return self.memory_monitor.get_current_status()
        return None
    
    def __del__(self):
        try:
            if hasattr(self, 'memory_monitor') and self.memory_monitor:
                from memory_monitor import stop_memory_monitoring
                stop_memory_monitoring()
        except:
            pass

model_manager = None

def get_model_manager(gpu_memory_limit=20):
    global model_manager
    if model_manager is None:
        model_manager = ModelManager(gpu_memory_limit)
    return model_manager

def initialize_models(gpu_memory_limit=20):
    return get_model_manager(gpu_memory_limit)

if __name__ == "__main__":
    print("æ¸¬è©¦æ¨¡å‹ç®¡ç†å™¨...")
    manager = initialize_models(gpu_memory_limit=20)
    info = manager.get_device_info()
    print("è¨­å‚™ä¿¡æ¯:", info)
    
    memory_status = manager.get_memory_status()
    if memory_status:
        print("è¨˜æ†¶é«”ç‹€æ…‹:", memory_status)