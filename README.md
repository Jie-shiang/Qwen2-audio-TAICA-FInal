# ğŸ—£ï¸ æ™ºèƒ½èªè¨€å­¸ç¿’åŠ©æ•™ç³»çµ±

**æ¸…è¯å¤§å­¸é›»æ©Ÿæ‰€ 113061529 æ¥Šå‚‘ç¿” Final Project**

> åŸºæ–¼ Whisper + Qwen2-Audio çš„å¤šæ¨¡æ…‹èªè¨€å­¸ç¿’å¹³å°ï¼Œæä¾›å€‹æ€§åŒ–ç™¼éŸ³è©•ä¼°èˆ‡å³æ™‚å°è©±ç·´ç¿’

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org)
[![Gradio](https://img.shields.io/badge/Gradio-4.0+-orange.svg)](https://gradio.app)
[![License](https://img.shields.io/badge/License-Academic-green.svg)](LICENSE)

## ğŸ“‹ ç›®éŒ„

- [âœ¨ å°ˆæ¡ˆäº®é»](#-å°ˆæ¡ˆäº®é»)
- [ğŸ¯ åŠŸèƒ½ç‰¹è‰²](#-åŠŸèƒ½ç‰¹è‰²)
- [ğŸ—ï¸ ç³»çµ±æ¶æ§‹](#ï¸-ç³»çµ±æ¶æ§‹)
- [ğŸ“ å°ˆæ¡ˆçµæ§‹](#-å°ˆæ¡ˆçµæ§‹)
- [ğŸš€ å¿«é€Ÿé–‹å§‹](#-å¿«é€Ÿé–‹å§‹)
- [ğŸ’» ç³»çµ±éœ€æ±‚](#-ç³»çµ±éœ€æ±‚)
- [ğŸ”§ æ¨¡çµ„è©³è§£](#-æ¨¡çµ„è©³è§£)
- [ğŸ“± ä½¿ç”¨æŒ‡å—](#-ä½¿ç”¨æŒ‡å—)
- [âš™ï¸ é«˜ç´šé…ç½®](#ï¸-é«˜ç´šé…ç½®)
- [ğŸ› æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)
- [ğŸ“Š æ€§èƒ½å„ªåŒ–](#-æ€§èƒ½å„ªåŒ–)
- [ğŸ¤ é–‹ç™¼æŒ‡å—](#-é–‹ç™¼æŒ‡å—)
- [ğŸ“„ æŠ€è¡“æ–‡æª”](#-æŠ€è¡“æ–‡æª”)
- [ğŸ™ è‡´è¬](#-è‡´è¬)

## âœ¨ å°ˆæ¡ˆäº®é»

### ğŸ§  å¤šæ¨¡æ…‹AIæ¶æ§‹
- **WhisperèªéŸ³è­˜åˆ¥**: OpenAIé ‚ç´šèªéŸ³è½‰æ–‡å­—æ¨¡å‹
- **Qwen2-Audioåˆ†æ**: é˜¿é‡Œå·´å·´å¤šæ¨¡æ…‹èªè¨€æ¨¡å‹ï¼Œç›´æ¥éŸ³é »ç†è§£
- **æ™ºèƒ½é™ç´šæ©Ÿåˆ¶**: GPUè¨˜æ†¶é«”ä¸è¶³æ™‚è‡ªå‹•åˆ‡æ›åˆ°CPUç°¡åŒ–æ¨¡å¼

### ğŸ¯ å€‹æ€§åŒ–å­¸ç¿’é«”é©—
- **5ç´šé›£åº¦ç³»çµ±**: å¾åˆå­¸è€…(TOEIC 250)åˆ°é«˜ç´š(TOEIC 905+)
- **6å¤§å ´æ™¯æ¨¡æ“¬**: æ©Ÿå ´ã€é¤å»³ã€é¢è©¦ã€ç¤¾äº¤ã€é†«ç™‚ã€å­¸è¡“
- **å³æ™‚ç™¼éŸ³è©•åˆ†**: ç¶œåˆç™¼éŸ³æº–ç¢ºåº¦èˆ‡æµæš¢åº¦è©•ä¼°
- **æ™ºèƒ½å›é¥‹èª¿æ•´**: åŸºæ–¼å­¸ç¿’è€…æ°´å¹³çš„å€‹æ€§åŒ–å»ºè­°

### âš¡ æ™ºèƒ½è³‡æºç®¡ç†
- **å‹•æ…‹è¨˜æ†¶é«”ç›£æ§**: å¯¦æ™‚GPU/CPUä½¿ç”¨é‡è¿½è¹¤
- **è‡ªé©æ‡‰æ¨¡å‹è¼‰å…¥**: æ ¹æ“šç¡¬é«”é…ç½®è‡ªå‹•å„ªåŒ–
- **ç·Šæ€¥æ¸…ç†æ©Ÿåˆ¶**: é˜²æ­¢è¨˜æ†¶é«”æº¢å‡ºçš„ä¿è­·æªæ–½

### ğŸ¨ ç¾ä»£åŒ–UIè¨­è¨ˆ
- **éŸ¿æ‡‰å¼ç•Œé¢**: æ”¯æ´æ¡Œé¢ç«¯èˆ‡è¡Œå‹•è£ç½®
- **æ¯›ç»ç’ƒæ•ˆæœ**: ç¾ä»£åŒ–è¦–è¦ºè¨­è¨ˆ
- **ç„¡éšœç¤™æ”¯æŒ**: å‹å–„çš„ç”¨æˆ¶é«”é©—è¨­è¨ˆ

## ğŸ¯ åŠŸèƒ½ç‰¹è‰²

### ğŸ“š é›™æ¨¡å¼å­¸ç¿’ç³»çµ±

#### ğŸ­ é è¨­å ´æ™¯å°è©±
```
âœˆï¸ æ©Ÿå ´å°è©±    - é€šé—œã€ç™»æ©Ÿã€è­·ç…§æª¢æŸ¥æƒ…å¢ƒ
ğŸ½ï¸ é¤å»³é»é¤    - é»é¤ã€è©¢å•èœå–®ã€çµå¸³å°è©±  
ğŸ’¼ æ±‚è·é¢è©¦    - å·¥ä½œé¢è©¦å•ç­”èˆ‡è‡ªæˆ‘ä»‹ç´¹
ğŸ¤ æ—¥å¸¸ç¤¾äº¤    - å•å€™ã€é–’èŠã€ç¤¾äº¤äº’å‹•
ğŸ¥ é†«ç™‚è«®è©¢    - ç—…æƒ…æè¿°ã€é†«ç™‚å°è©±ç·´ç¿’
ğŸ“š å­¸è¡“è¨è«–    - èª²å ‚ç™¼è¨€ã€ç ”è¨æœƒäº’å‹•
```

#### ğŸ’­ è‡ªç”±å°è©±æ¨¡å¼
- ç”¨æˆ¶è‡ªå®šç¾©å ´æ™¯å’Œè©±é¡Œ
- é–‹æ”¾å¼å°è©±ç·´ç¿’
- éˆæ´»çš„å­¸ç¿’å…§å®¹

### ğŸ§ å¤šå±¤æ¬¡èªéŸ³åˆ†æ

#### ğŸ” åŸºç¤åˆ†æ (ç°¡åŒ–æ¨¡å¼)
- èªéŸ³è­˜åˆ¥æº–ç¢ºåº¦è©•ä¼°
- åŸºæœ¬ç™¼éŸ³è©•åˆ†ç®—æ³•
- æµæš¢åº¦çµ±è¨ˆåˆ†æ

#### ğŸ§  é€²éšåˆ†æ (Audio-LLMæ¨¡å¼)
- ç›´æ¥éŸ³é »å…§å®¹ç†è§£
- ä¸Šä¸‹æ–‡ç›¸é—œçš„å›é¥‹
- ç´°ç·»çš„ç™¼éŸ³ç³¾æ­£å»ºè­°

### ğŸ“Š å€‹æ€§åŒ–é›£åº¦ç³»çµ±

| é›£åº¦ç´šåˆ¥ | TOEICåˆ†æ•¸ | è©•ä¼°æ¨™æº– | å›é¥‹ç‰¹è‰² |
|---------|----------|----------|---------|
| åˆå­¸è€… | 250-400 | åŸºç¤ç™¼éŸ³æ¸…æ™°åº¦ | æ¥µåº¦é¼“å‹µæ€§ (+15åˆ†èª¿æ•´) |
| åˆç´š | 405-600 | åŸºæœ¬å°è©±æµæš¢æ€§ | é¼“å‹µæ€§ (+10åˆ†èª¿æ•´) |
| ä¸­ç´š | 605-780 | èªæ³•æº–ç¢ºåº¦èˆ‡è‡ªç„¶åº¦ | å¹³è¡¡æ€§ (æ¨™æº–è©•åˆ†) |
| ä¸­é«˜ç´š | 785-900 | æ…£ç”¨èªèˆ‡ç´°ç·»ç™¼éŸ³ | å»ºè¨­æ€§ (-5åˆ†èª¿æ•´) |
| é«˜ç´š | 905+ | å°ˆæ¥­ç´šæµæš¢åº¦ | è©³ç´°åˆ†æ (-10åˆ†èª¿æ•´) |

### ğŸ”§ é€²éšåŠŸèƒ½è¨­å®š

#### ğŸ¯ ç™¼éŸ³é‡é»é—œæ³¨
- **å­éŸ³ç™¼éŸ³**: æ¸…æ™°åº¦èˆ‡æº–ç¢ºæ€§
- **æ¯éŸ³ç™¼éŸ³**: éŸ³ä½æº–ç¢ºåº¦
- **é€£éŸ³æŠ€å·§**: è‡ªç„¶èªæµè™•ç†
- **é‡éŸ³æ¨¡å¼**: å–®å­—èˆ‡å¥å­é‡éŸ³
- **èªèª¿è®ŠåŒ–**: å‡é™èª¿èˆ‡æƒ…æ„Ÿè¡¨é”
- **ç¯€å¥æ§åˆ¶**: èªé€Ÿèˆ‡åœé “

#### ğŸŒ å£éŸ³åå¥½è¨­å®š
- **ç¾å¼è‹±æ–‡**: General Americanç™¼éŸ³æ¨™æº–
- **è‹±å¼è‹±æ–‡**: Received Pronunciationæ¨™æº–
- **å½ˆæ€§æ¨¡å¼**: ä¸æŒ‡å®šç‰¹å®šå£éŸ³

#### ğŸ“ˆ å­¸ç¿’è¿½è¹¤åŠŸèƒ½
- **é€²åº¦è¨˜éŒ„**: è‡ªå‹•ä¿å­˜ç·´ç¿’æ­·ç¨‹
- **çµ±è¨ˆåˆ†æ**: ç™¼éŸ³æ”¹å–„è¶¨å‹¢åœ–è¡¨
- **æ­·å²å°æ¯”**: èˆ‡æ¨™æº–ç™¼éŸ³æ¯”è¼ƒ
- **åŒ¯å‡ºåŠŸèƒ½**: å­¸ç¿’è¨˜éŒ„åŒ¯å‡º

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

```mermaid
graph TB
    A[ç”¨æˆ¶èªéŸ³è¼¸å…¥] --> B[WhisperèªéŸ³è­˜åˆ¥]
    B --> C{è¨˜æ†¶é«”æª¢æŸ¥}
    C -->|è¶³å¤ | D[Qwen2-Audioåˆ†æ]
    C -->|ä¸è¶³| E[ç°¡åŒ–åˆ†ææ¨¡å¼]
    D --> F[è©³ç´°ç™¼éŸ³å›é¥‹]
    E --> G[åŸºç¤ç™¼éŸ³è©•åˆ†]
    F --> H[å€‹æ€§åŒ–å›æ‡‰ç”Ÿæˆ]
    G --> H
    H --> I[ç”¨æˆ¶ç•Œé¢å±•ç¤º]
    
    J[è¨˜æ†¶é«”ç›£æ§å™¨] --> C
    K[é›£åº¦é…ç½®ç³»çµ±] --> F
    K --> G
    L[å ´æ™¯ç®¡ç†å™¨] --> H
```

### ğŸ”„ æ™ºèƒ½é™ç´šæ©Ÿåˆ¶

```python
# è‡ªå‹•æ¨¡å‹é¸æ“‡æµç¨‹
if GPU_memory > 10GB:
    load_qwen2_audio_model(dtype=float16)
elif GPU_memory > 6GB:
    load_qwen2_audio_model(dtype=float16, quantized=True)
else:
    use_simplified_analysis_mode()
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
Qwen2-audio-TAICA-Final/
â”œâ”€â”€ ğŸ“„ app.py                 # ä¸»æ‡‰ç”¨ç¨‹å¼èˆ‡Gradioç•Œé¢
â”œâ”€â”€ ğŸ§  models.py              # AIæ¨¡å‹ç®¡ç†èˆ‡GPUå„ªåŒ–
â”œâ”€â”€ âš™ï¸ processors.py          # éŸ³é »è™•ç†èˆ‡èªè¨€åˆ†ææ ¸å¿ƒ
â”œâ”€â”€ ğŸ“Š memory_monitor.py      # æ™ºèƒ½è¨˜æ†¶é«”ç›£æ§ç³»çµ±
â”œâ”€â”€ ğŸ¨ styles.css             # ç¾ä»£åŒ–UIæ¨£å¼è¨­è¨ˆ
â”œâ”€â”€ ğŸ“‹ requirements.txt       # Pythonä¾è³´å¥—ä»¶æ¸…å–®
â”œâ”€â”€ ğŸ“š README.md              # å°ˆæ¡ˆèªªæ˜æ–‡æª”
â””â”€â”€ ğŸ“ scenario_images/       # å ´æ™¯é…åœ–è³‡æº
    â”œâ”€â”€ airport.jpg           # æ©Ÿå ´å ´æ™¯åœ–
    â”œâ”€â”€ restaurant.jpg        # é¤å»³å ´æ™¯åœ–
    â”œâ”€â”€ interview.jpg         # é¢è©¦å ´æ™¯åœ–
    â”œâ”€â”€ socializing.jpg       # ç¤¾äº¤å ´æ™¯åœ–
    â”œâ”€â”€ medical.jpg           # é†«ç™‚å ´æ™¯åœ–
    â””â”€â”€ academic.jpg          # å­¸è¡“å ´æ™¯åœ–
```

### ğŸ“‚ æ ¸å¿ƒæ¨¡çµ„è·è²¬

| æ¨¡çµ„ | ä¸»è¦åŠŸèƒ½ | æŠ€è¡“ç‰¹è‰² |
|------|----------|----------|
| `models.py` | AIæ¨¡å‹ç®¡ç† | GPUè‡ªå‹•æª¢æ¸¬ã€è¨˜æ†¶é«”å„ªåŒ–ã€é™ç´šæ©Ÿåˆ¶ |
| `processors.py` | èªéŸ³è™•ç†åˆ†æ | å¤šå±¤æ¬¡åˆ†æã€é›£åº¦èª¿æ•´ã€å ´æ™¯é©é… |
| `app.py` | ç”¨æˆ¶ç•Œé¢é‚è¼¯ | éŸ¿æ‡‰å¼è¨­è¨ˆã€äº‹ä»¶è™•ç†ã€ç‹€æ…‹ç®¡ç† |
| `memory_monitor.py` | ç³»çµ±ç›£æ§ | å¯¦æ™‚ç›£æ§ã€è‡ªå‹•æ¸…ç†ã€ç·Šæ€¥ä¿è­· |
| `styles.css` | è¦–è¦ºè¨­è¨ˆ | æ¯›ç»ç’ƒæ•ˆæœã€éŸ¿æ‡‰å¼å¸ƒå±€ã€ç„¡éšœç¤™æ”¯æŒ |

## ğŸš€ å¿«é€Ÿé–‹å§‹

### 1ï¸âƒ£ ç’°å¢ƒè¨­ç½®

```bash
# å…‹éš†å°ˆæ¡ˆ
git clone <your-repo-url>
cd Qwen2-audio-TAICA-Final

# å‰µå»ºè™›æ“¬ç’°å¢ƒ
conda create --name language_assistant python=3.10
conda activate language_assistant

# å®‰è£PyTorch (CUDAç‰ˆæœ¬)
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia

# å®‰è£å…¶ä»–ä¾è³´
pip install -r requirements.txt
```

### 2ï¸âƒ£ æ¨¡å‹æº–å‚™

```bash
# è‡ªå‹•ä¸‹è¼‰æ¨¡å‹ (é¦–æ¬¡é‹è¡Œæ™‚)
python models.py

# æˆ–æ‰‹å‹•ä¸‹è¼‰ (å¯é¸)
huggingface-cli download openai/whisper-medium
huggingface-cli download Qwen/Qwen2-Audio-7B-Instruct
```

### 3ï¸âƒ£ å•Ÿå‹•æ‡‰ç”¨

```bash
# åŸºæœ¬å•Ÿå‹•
python app.py

# æŒ‡å®šGPUè¨˜æ†¶é«”é™åˆ¶ (é è¨­20GB)
GPU_MEMORY_LIMIT=16 python app.py

# CPUæ¨¡å¼ (ç„¡GPUæ™‚)
CUDA_VISIBLE_DEVICES="" python app.py
```

### 4ï¸âƒ£ è¨ªå•ç•Œé¢

- **æœ¬åœ°è¨ªå•**: http://localhost:7861
- **ç¶²è·¯åˆ†äº«**: å•Ÿå‹•æ™‚æœƒè‡ªå‹•ç”Ÿæˆ Gradio åˆ†äº«é€£çµ
- **æ”¯æ´è¨­å‚™**: æ¡Œé¢ç€è¦½å™¨ã€å¹³æ¿ã€æ‰‹æ©Ÿ

## ğŸ’» ç³»çµ±éœ€æ±‚

### ğŸŸ¢ æœ€ä½é…ç½®

| çµ„ä»¶ | éœ€æ±‚ | èªªæ˜ |
|------|------|------|
| **Python** | 3.10+ | æ”¯æ´æœ€æ–°èªè¨€ç‰¹æ€§ |
| **è¨˜æ†¶é«”** | 8GB RAM | åŸºæœ¬æ¨¡å‹è¼‰å…¥éœ€æ±‚ |
| **å„²å­˜ç©ºé–“** | 15GB å¯ç”¨ç©ºé–“ | æ¨¡å‹æª”æ¡ˆç´„10GB |
| **ç¶²è·¯** | å¯¬é »é€£ç·š | é¦–æ¬¡ä¸‹è¼‰æ¨¡å‹éœ€è¦ |
| **éŸ³é »è¨­å‚™** | éº¥å…‹é¢¨ | éŒ„éŸ³åŠŸèƒ½å¿…éœ€ |

### â­ å»ºè­°é…ç½®

| çµ„ä»¶ | å»ºè­°è¦æ ¼ | æ€§èƒ½æå‡ |
|------|----------|----------|
| **GPU** | RTX 4070 / RTX 3080 (8GB+ VRAM) | 10-20x åŠ é€Ÿ |
| **è¨˜æ†¶é«”** | 16GB+ RAM | æ›´å¥½çš„å¤šå·¥è™•ç† |
| **CPU** | 8æ ¸å¿ƒ+ ç¾ä»£è™•ç†å™¨ | CPUé™ç´šæ¨¡å¼æ€§èƒ½ |
| **å„²å­˜** | SSD å›ºæ…‹ç¡¬ç¢Ÿ | æ›´å¿«çš„æ¨¡å‹è¼‰å…¥ |
| **éŸ³é »** | é«˜å“è³ªUSBéº¥å…‹é¢¨ | æ›´å¥½çš„èªéŸ³è­˜åˆ¥ |

### ğŸ”§ GPUæ”¯æ´æƒ…æ³

| GPUå‹è™Ÿ | VRAM | Audio-LLMæ”¯æ´ | æ¨è–¦è¨­å®š |
|---------|------|---------------|----------|
| RTX 4090 | 24GB | âœ… å®Œå…¨æ”¯æ´ | float16, å…¨åŠŸèƒ½ |
| RTX 4080 | 16GB | âœ… å®Œå…¨æ”¯æ´ | float16, å…¨åŠŸèƒ½ |
| RTX 4070 | 12GB | âœ… å®Œå…¨æ”¯æ´ | float16, å»ºè­°é™åˆ¶18GB |
| RTX 3080 | 10GB | âš ï¸ éƒ¨åˆ†æ”¯æ´ | float16, é‡åŒ–æ¨¡å¼ |
| RTX 3070 | 8GB | âš ï¸ åŸºç¤æ”¯æ´ | ç°¡åŒ–æ¨¡å¼ |
| GTX 1660 | 6GB | âŒ CPUé™ç´š | Whisper only |

## ğŸ”§ æ¨¡çµ„è©³è§£

### ğŸ§  models.py - AIæ¨¡å‹ç®¡ç†ä¸­å¿ƒ

```python
class ModelManager:
    """
    çµ±ä¸€ç®¡ç†æ‰€æœ‰AIæ¨¡å‹ï¼ŒåŒ…å«ï¼š
    - GPU/CPUè‡ªå‹•æª¢æ¸¬èˆ‡é…ç½®
    - WhisperèªéŸ³è­˜åˆ¥æ¨¡å‹è¼‰å…¥
    - Qwen2-Audioå¤šæ¨¡æ…‹æ¨¡å‹ç®¡ç†
    - æ™ºèƒ½è¨˜æ†¶é«”å„ªåŒ–èˆ‡ç›£æ§
    """
```

#### æ ¸å¿ƒç‰¹æ€§
- **è‡ªé©æ‡‰ç¡¬é«”æª¢æ¸¬**: è‡ªå‹•è­˜åˆ¥æœ€ä½³GPUé…ç½®
- **è¨˜æ†¶é«”å®‰å…¨æ©Ÿåˆ¶**: é˜²æ­¢OOMéŒ¯èª¤çš„å¤šå±¤ä¿è­·
- **æ¨¡å‹ç†±åˆ‡æ›**: é‹è¡Œæ™‚å‹•æ…‹èª¿æ•´æ¨¡å‹ç²¾åº¦
- **è³‡æºæ¸…ç†**: æ™ºèƒ½åƒåœ¾å›æ”¶èˆ‡è¨˜æ†¶é«”é‡‹æ”¾

### âš™ï¸ processors.py - èªéŸ³è™•ç†èˆ‡åˆ†ææ ¸å¿ƒ

```python
class AudioProcessor:
    """
    è™•ç†èªéŸ³åˆ†æçš„æ ¸å¿ƒé‚è¼¯ï¼š
    - å¤šå±¤æ¬¡èªéŸ³è­˜åˆ¥èˆ‡ç†è§£
    - åŸºæ–¼é›£åº¦çš„è©•åˆ†èª¿æ•´ç®—æ³•
    - å€‹æ€§åŒ–å›é¥‹å…§å®¹ç”Ÿæˆ
    - ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å°è©±ç®¡ç†
    """
```

#### åˆ†æå±¤æ¬¡æ¶æ§‹

1. **èªéŸ³è­˜åˆ¥å±¤** (Whisper)
   - é«˜ç²¾åº¦èªéŸ³è½‰æ–‡å­—
   - å¤šèªè¨€æ”¯æ´èƒ½åŠ›
   - é›œéŸ³ç’°å¢ƒé©æ‡‰

2. **èªç¾©ç†è§£å±¤** (Qwen2-Audio)  
   - ç›´æ¥éŸ³é »å…§å®¹åˆ†æ
   - èªèª¿æƒ…æ„Ÿè­˜åˆ¥
   - ç™¼éŸ³ç´°ç¯€è©•ä¼°

3. **è©•åˆ†èª¿æ•´å±¤** (Difficulty-Aware)
   - åŸºæ–¼TOEICç´šåˆ¥çš„å‹•æ…‹è©•åˆ†
   - å­¸ç¿’è€…é€²åº¦è¿½è¹¤
   - å€‹æ€§åŒ–å»ºè­°ç”Ÿæˆ

### ğŸ“Š memory_monitor.py - æ™ºèƒ½è³‡æºç›£æ§

```python
class MemoryMonitor:
    """
    å¯¦æ™‚ç³»çµ±è³‡æºç›£æ§ï¼š
    - GPUè¨˜æ†¶é«”ä½¿ç”¨è¿½è¹¤
    - CPUè³‡æºç›£æ§
    - è‡ªå‹•ç·Šæ€¥æ¸…ç†æ©Ÿåˆ¶
    - é€²ç¨‹ä¿è­·èˆ‡æ¢å¾©
    """
```

#### ä¿è­·æ©Ÿåˆ¶å±¤ç´š

1. **é è­¦éšæ®µ** (80% ä½¿ç”¨ç‡)
   - è¨˜æ†¶é«”ä½¿ç”¨è­¦å‘Š
   - è‡ªå‹•åƒåœ¾å›æ”¶
   - æ¨¡å‹ç²¾åº¦èª¿æ•´

2. **ä¿è­·éšæ®µ** (90% ä½¿ç”¨ç‡)
   - ç·Šæ€¥è¨˜æ†¶é«”æ¸…ç†
   - æ¨¡å‹å¸è¼‰é‡è¼‰
   - åŠŸèƒ½é™ç´šè™•ç†

3. **ç·Šæ€¥éšæ®µ** (95%+ ä½¿ç”¨ç‡)
   - å¼·åˆ¶ç¨‹åºçµ‚æ­¢
   - æ•¸æ“šè‡ªå‹•ä¿å­˜
   - ç³»çµ±ç‹€æ…‹è¨˜éŒ„

### ğŸ¨ styles.css - ç¾ä»£åŒ–UIè¨­è¨ˆç³»çµ±

#### è¨­è¨ˆèªè¨€ç‰¹è‰²

- **æ¯›ç»ç’ƒæ“¬æ…‹è¨­è¨ˆ**: `backdrop-filter: blur()` ç¾ä»£è¦–è¦ºæ•ˆæœ
- **æ¼¸å±¤é…è‰²æ–¹æ¡ˆ**: è±å¯Œçš„é¡è‰²å±¤æ¬¡èˆ‡å“ç‰Œä¸€è‡´æ€§
- **éŸ¿æ‡‰å¼å¸ƒå±€**: è·¨è¨­å‚™å®Œç¾é©é…
- **ç„¡éšœç¤™æ”¯æŒ**: WCAG 2.1 æ¨™æº–éµå¾ª

```css
/* æ ¸å¿ƒè¨­è¨ˆç³»çµ± */
.main-container {
    background: rgba(255, 255, 255, 0.98);
    backdrop-filter: blur(15px);
    border: 2px solid rgba(102, 126, 234, 0.15);
    border-radius: 20px;
    box-shadow: 
        0 20px 40px rgba(0, 0, 0, 0.1),
        inset 0 1px 0 rgba(255, 255, 255, 0.8);
}
```

## ğŸ“± ä½¿ç”¨æŒ‡å—

### ğŸ¯ é è¨­å ´æ™¯æ¨¡å¼æ“ä½œæµç¨‹

#### 1. ç³»çµ±è¨­å®š
```
1ï¸âƒ£ é¸æ“‡å­¸ç¿’èªè¨€ (ç›®å‰æ”¯æ´è‹±æ–‡)
2ï¸âƒ£ è¨­å®šé›£åº¦ç´šåˆ¥ (TOEIC 250-905+)  
3ï¸âƒ£ ç¢ºèªè¨­å®šä¸¦é¸æ“‡æ¨¡å¼
```

#### 2. å ´æ™¯é¸æ“‡
```
âœˆï¸ æ©Ÿå ´å°è©± - è­·ç…§æª¢æŸ¥ã€ç™»æ©Ÿç¨‹åº
ğŸ½ï¸ é¤å»³é»é¤ - èœå–®è©¢å•ã€é»é¤çµå¸³  
ğŸ’¼ æ±‚è·é¢è©¦ - è‡ªæˆ‘ä»‹ç´¹ã€å•ç­”äº’å‹•
ğŸ¤ æ—¥å¸¸ç¤¾äº¤ - å•å€™é–’èŠã€ç¤¾äº¤å°è©±
ğŸ¥ é†«ç™‚è«®è©¢ - ç—‡ç‹€æè¿°ã€é†«ç™‚æºé€š
ğŸ“š å­¸è¡“è¨è«– - èª²å ‚ç™¼è¨€ã€å­¸è¡“äº¤æµ
```

#### 3. å°è©±ç·´ç¿’
```
ğŸ¤ é»æ“Šéº¥å…‹é¢¨åœ–æ¨™é–‹å§‹éŒ„éŸ³
ğŸ”´ èªªå‡ºæ‚¨çš„å›æ‡‰ (å»ºè­°5-15ç§’)
â¹ï¸ åœæ­¢éŒ„éŸ³ä¸¦ç­‰å¾…åˆ†æ
ğŸ“Š æŸ¥çœ‹è©³ç´°ç™¼éŸ³è©•ä¼°èˆ‡å»ºè­°
ğŸ”„ æ ¹æ“šå»ºè­°é€²è¡Œæ”¹é€²ç·´ç¿’
```

### ğŸ’­ è‡ªç”±å°è©±æ¨¡å¼

#### è‡ªå®šç¾©å ´æ™¯ç¤ºä¾‹

```markdown
# å•†å‹™æœƒè­°å ´æ™¯
"æˆ‘æƒ³ç·´ç¿’åœ¨åœ‹éš›å•†å‹™æœƒè­°ä¸­ç™¼è¨€ï¼Œ
åŒ…æ‹¬æå‡ºå»ºè­°ã€è¡¨é”æ„è¦‹å’Œè©¢å•å•é¡Œçš„å°è©±ã€‚"

# æ—…éŠè«®è©¢å ´æ™¯  
"æ¨¡æ“¬åœ¨æ—…éŠæœå‹™ä¸­å¿ƒè©¢å•æ™¯é»è³‡è¨Šã€
äº¤é€šæ–¹å¼å’Œä½å®¿å»ºè­°çš„å°è©±ã€‚"

# å­¸è¡“ç ”è¨å ´æ™¯
"ç·´ç¿’åœ¨å­¸è¡“ç ”è¨æœƒä¸­æå•ã€
å›æ‡‰ä»–äººè§€é»ä¸¦é€²è¡Œå°ˆæ¥­è¨è«–ã€‚"
```

### ğŸ”§ é€²éšåŠŸèƒ½ä½¿ç”¨

#### ç™¼éŸ³é‡é»é—œæ³¨è¨­å®š

```python
# å­éŸ³ç™¼éŸ³é‡é»
focus_areas = [
    "théŸ³ (think, that)",
    "r/lå€åˆ¥ (right, light)", 
    "v/wå€åˆ¥ (very, worry)",
    "æ¸…æ¿éŸ³å°æ¯” (pat, bat)"
]

# èªèª¿ç·´ç¿’é‡é»
intonation_focus = [
    "ç–‘å•å¥ä¸Šå‡èª¿",
    "é™³è¿°å¥ä¸‹é™èª¿", 
    "å¼·èª¿èªèª¿è®ŠåŒ–",
    "æƒ…æ„Ÿè¡¨é”èªèª¿"
]
```

#### å­¸ç¿’è¿½è¹¤åŠŸèƒ½

- **ç·´ç¿’æ­·ç¨‹**: è‡ªå‹•è¨˜éŒ„æ¯æ¬¡ç·´ç¿’çš„æ™‚é–“ã€å ´æ™¯ã€å¾—åˆ†
- **é€²æ­¥è¶¨å‹¢**: åœ–è¡¨åŒ–é¡¯ç¤ºç™¼éŸ³æ”¹å–„è»Œè·¡  
- **å¼±é …åˆ†æ**: AIè­˜åˆ¥éœ€è¦åŠ å¼·çš„ç™¼éŸ³è¦é»
- **ç›®æ¨™è¨­å®š**: æ ¹æ“šTOEICç´šåˆ¥è¨­å®šå­¸ç¿’ç›®æ¨™

## âš™ï¸ é«˜ç´šé…ç½®

### ğŸš€ æ€§èƒ½èª¿å„ªåƒæ•¸

```python
# models.py ä¸­çš„é—œéµé…ç½®
GPU_MEMORY_LIMIT = 20  # GPUè¨˜æ†¶é«”é™åˆ¶ (GB)
CHECK_INTERVAL = 3     # è¨˜æ†¶é«”æª¢æŸ¥é–“éš” (ç§’)
WHISPER_MODEL_SIZE = "medium"  # base/small/medium/large
AUDIO_LLM_PRECISION = "float16"  # float32/float16

# processors.py ä¸­çš„åˆ†æåƒæ•¸  
SCORE_ADJUSTMENT_RANGE = (-10, +15)  # é›£åº¦èª¿æ•´ç¯„åœ
ANALYSIS_DETAIL_LEVELS = 3  # å›é¥‹è©³ç´°ç¨‹åº¦
CONVERSATION_HISTORY_LIMIT = 10  # å°è©±æ­·å²ä¿ç•™æ•¸é‡
```

### ğŸ”§ ç’°å¢ƒè®Šæ•¸é…ç½®

```bash
# GPUè¨­å®š
export CUDA_VISIBLE_DEVICES="0"          # æŒ‡å®šGPU
export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:512"

# æ¨¡å‹è·¯å¾‘ (å¯é¸)
export WHISPER_CACHE_DIR="/path/to/whisper/models"
export HF_HOME="/path/to/huggingface/cache"

# è¨˜æ†¶é«”é™åˆ¶
export GPU_MEMORY_LIMIT="16"             # GPUè¨˜æ†¶é«”é™åˆ¶(GB)
export CPU_MEMORY_LIMIT="32"             # CPUè¨˜æ†¶é«”é™åˆ¶(GB)

# æ‡‰ç”¨è¨­å®š
export GRADIO_SERVER_PORT="7861"         # æœå‹™ç«¯å£
export GRADIO_SHARE="true"               # æ˜¯å¦ç”¢ç”Ÿåˆ†äº«é€£çµ
```

### ğŸ“Š ç›£æ§èˆ‡æ—¥èªŒé…ç½®

```python
# å•Ÿç”¨è©³ç´°æ—¥èªŒ
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('language_assistant.log'),
        logging.StreamHandler()
    ]
)

# è¨˜æ†¶é«”ç›£æ§è‡ªå®šç¾©
monitor = MemoryMonitor(
    gpu_limit_gb=18,      # è‡ªå®šç¾©GPUé™åˆ¶
    cpu_limit_gb=24,      # è‡ªå®šç¾©CPUé™åˆ¶  
    check_interval=2      # æ›´é »ç¹çš„æª¢æŸ¥
)
```

## ğŸ› æ•…éšœæ’é™¤

### â— å¸¸è¦‹å•é¡Œè§£æ±ºæ–¹æ¡ˆ

#### 1. æ¨¡å‹è¼‰å…¥å¤±æ•—

```bash
# å•é¡Œ: ç¶²è·¯ä¸‹è¼‰è¶…æ™‚
è§£æ±ºæ–¹æ¡ˆ:
1. æª¢æŸ¥ç¶²è·¯é€£ç·šç©©å®šæ€§
2. ä½¿ç”¨ä»£ç†æˆ–VPN (å¦‚æœéœ€è¦)
3. æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹æ–‡ä»¶

# æ‰‹å‹•ä¸‹è¼‰æŒ‡ä»¤
huggingface-cli download --resume-download openai/whisper-medium
huggingface-cli download --resume-download Qwen/Qwen2-Audio-7B-Instruct
```

#### 2. GPUè¨˜æ†¶é«”ä¸è¶³

```python
# å•é¡Œ: torch.cuda.OutOfMemoryError
è§£æ±ºæ–¹æ¡ˆ:
1. é™ä½GPU_MEMORY_LIMITè¨­å®š
2. é—œé–‰å…¶ä»–GPUç¨‹åº
3. ä½¿ç”¨CPUæ¨¡å¼

# å¼·åˆ¶CPUæ¨¡å¼
CUDA_VISIBLE_DEVICES="" python app.py
```

#### 3. éŸ³é »è­˜åˆ¥éŒ¯èª¤

```bash
# å•é¡Œ: éº¥å…‹é¢¨ç„¡æ³•éŒ„éŸ³
è§£æ±ºæ–¹æ¡ˆ:
1. æª¢æŸ¥ç€è¦½å™¨éº¥å…‹é¢¨æ¬Šé™
2. æ¸¬è©¦éº¥å…‹é¢¨ç¡¬é«”åŠŸèƒ½  
3. ç¢ºèªéŸ³é »æ ¼å¼æ”¯æ´

# ç€è¦½å™¨æ¬Šé™è¨­å®š
Chrome: è¨­å®š > éš±ç§æ¬Šå’Œå®‰å…¨æ€§ > ç¶²ç«™è¨­å®š > éº¥å…‹é¢¨
Firefox: åå¥½è¨­å®š > éš±ç§æ¬Šèˆ‡å®‰å…¨æ€§ > æ¬Šé™ > éº¥å…‹é¢¨
```

#### 4. ç•Œé¢é¡¯ç¤ºç•°å¸¸

```bash
# å•é¡Œ: CSSæ¨£å¼æœªè¼‰å…¥
è§£æ±ºæ–¹æ¡ˆ:
1. ç¢ºèª styles.css æ–‡ä»¶å­˜åœ¨
2. æª¢æŸ¥æ–‡ä»¶æ¬Šé™è¨­å®š
3. æ¸…é™¤ç€è¦½å™¨ç·©å­˜

# æª¢æŸ¥æ–‡ä»¶
ls -la styles.css
# æ‡‰è©²é¡¯ç¤ºæ–‡ä»¶å­˜åœ¨ä¸”å¯è®€å–
```

### ğŸ” èª¿è©¦æ¨¡å¼å•Ÿç”¨

```python
# å•Ÿç”¨è©³ç´°èª¿è©¦ä¿¡æ¯
debug_mode = True

if debug_mode:
    # 1. æ¨¡å‹è¼‰å…¥ç‹€æ…‹
    print("æ¨¡å‹ç®¡ç†å™¨ç‹€æ…‹:", model_manager.get_device_info())
    
    # 2. è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³
    print("è¨˜æ†¶é«”ç‹€æ…‹:", model_manager.get_memory_status())
    
    # 3. è™•ç†å™¨é…ç½®
    print("è™•ç†å™¨è¨­å®š:", processor.get_config())
    
    # 4. Gradioé™¤éŒ¯æ¨¡å¼
    demo.launch(debug=True, show_error=True)
```

### ğŸ“‹ ç³»çµ±è¨ºæ–·æŒ‡ä»¤

```bash
# GPUæª¢æ¸¬
nvidia-smi
python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"

# è¨˜æ†¶é«”æª¢æ¸¬  
free -h
python -c "import psutil; print(f'å¯ç”¨è¨˜æ†¶é«”: {psutil.virtual_memory().available/1024**3:.1f}GB')"

# Pythonç’°å¢ƒæª¢æ¸¬
python --version
pip list | grep -E "(torch|transformers|gradio|whisper)"

# æ¨¡å‹æ–‡ä»¶æª¢æ¸¬
find ~/.cache/huggingface -name "*whisper*" -type d
find ~/.cache/huggingface -name "*Qwen2-Audio*" -type d
```

## ğŸ“Š æ€§èƒ½å„ªåŒ–

### âš¡ é‹è¡Œé€Ÿåº¦å„ªåŒ–

#### 1. GPUåŠ é€Ÿé…ç½®

```python
# æœ€ä½³GPUè¨­å®š
torch.backends.cudnn.benchmark = True    # åŠ é€Ÿå·ç©é‹ç®—
torch.backends.cudnn.deterministic = False  # æå‡æ€§èƒ½
torch.set_float32_matmul_precision('high')   # æ··åˆç²¾åº¦é‹ç®—

# æ¨¡å‹å„ªåŒ–
model.half()  # ä½¿ç”¨float16ç²¾åº¦
model.eval()  # è©•ä¼°æ¨¡å¼
torch.no_grad()  # é—œé–‰æ¢¯åº¦è¨ˆç®—
```

#### 2. è¨˜æ†¶é«”ä½¿ç”¨å„ªåŒ–

```python
# è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥
BATCH_SIZE = 1                    # å–®æ‰¹æ¬¡è™•ç†
MAX_AUDIO_LENGTH = 30            # é™åˆ¶éŸ³é »é•·åº¦(ç§’)
CACHE_SIZE_LIMIT = 100           # ç·©å­˜é …ç›®é™åˆ¶
GARBAGE_COLLECTION_INTERVAL = 5  # GCé–“éš”(æ¬¡)

# è‡ªå‹•è¨˜æ†¶é«”æ¸…ç†
def auto_cleanup():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
```

#### 3. ä¸¦è¡Œè™•ç†å„ªåŒ–

```python
# å¤šç·šç¨‹è¨­å®š
torch.set_num_threads(4)              # CPUç·šç¨‹æ•¸
os.environ["OMP_NUM_THREADS"] = "4"   # OpenMPç·šç¨‹

# ç•°æ­¥è™•ç†
import asyncio
async def process_audio_async(audio_path):
    # éé˜»å¡éŸ³é »è™•ç†
    pass
```

### ğŸ“ˆ æ€§èƒ½ç›£æ§æŒ‡æ¨™

```python
import time
import psutil

class PerformanceMonitor:
    def __init__(self):
        self.start_time = time.time()
        self.process = psutil.Process()
    
    def get_metrics(self):
        return {
            "é‹è¡Œæ™‚é–“": time.time() - self.start_time,
            "CPUä½¿ç”¨ç‡": self.process.cpu_percent(),
            "è¨˜æ†¶é«”ä½¿ç”¨": self.process.memory_info().rss / 1024**2,
            "GPUè¨˜æ†¶é«”": torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0
        }
```

### ğŸ¯ æ¨¡å‹æ•ˆèƒ½èª¿ç¯€

| è¨­å®šé …ç›® | é«˜æ€§èƒ½ | å¹³è¡¡ | çœè³‡æº |
|---------|--------|------|---------|
| Whisperæ¨¡å‹ | large | medium | base |
| éŸ³é »LLMç²¾åº¦ | float16 | float16 | float32(CPU) |
| æ‰¹æ¬¡å¤§å° | 4 | 1 | 1 |
| è¨˜æ†¶é«”é™åˆ¶ | 24GB | 16GB | 8GB |
| å›é¥‹è©³ç´°åº¦ | å°ˆå®¶ç´š | è©³ç´° | åŸºæœ¬ |

## ğŸ¤ é–‹ç™¼æŒ‡å—

### ğŸ”§ æ“´å±•åŠŸèƒ½é–‹ç™¼

#### 1. æ–°å¢èªè¨€æ”¯æ´

```python
# processors.py ä¸­æ·»åŠ æ–°èªè¨€
SUPPORTED_LANGUAGES = {
    "è‹±æ–‡": "en",
    "ä¸­æ–‡": "zh",  # æ–°å¢ä¸­æ–‡æ”¯æ´
    "æ—¥æ–‡": "ja",  # æ–°å¢æ—¥æ–‡æ”¯æ´
    "éŸ“æ–‡": "ko"   # æ–°å¢éŸ“æ–‡æ”¯æ´
}

# ç‚ºæ–°èªè¨€é…ç½®Whisper
def transcribe_multilingual(audio_path, language):
    return whisper_model.transcribe(
        audio_path, 
        language=SUPPORTED_LANGUAGES[language],
        temperature=0.0
    )
```

#### 2. è‡ªå®šç¾©å ´æ™¯é–‹ç™¼

```python
# åœ¨ processors.py ä¸­æ–°å¢å ´æ™¯
def add_custom_scenario(scenario_name, prompt_template, responses):
    """
    æ–°å¢è‡ªå®šç¾©å ´æ™¯
    
    Args:
        scenario_name: å ´æ™¯åç¨±
        prompt_template: å ´æ™¯æç¤ºæ¨¡æ¿
        responses: å ´æ™¯å›æ‡‰åˆ—è¡¨
    """
    SCENARIO_PROMPTS[scenario_name] = prompt_template
    SCENARIO_RESPONSES[scenario_name] = responses

# ä½¿ç”¨ç¯„ä¾‹
add_custom_scenario(
    "éŠ€è¡Œæœå‹™ (Banking)",
    """You are a bank teller helping a customer with {level} English proficiency.
    Provide appropriate assistance and evaluate their banking vocabulary usage.""",
    [
        "How can I help you today?",
        "What type of account would you like to open?",
        "Please provide your identification.",
        "Your transaction has been completed."
    ]
)
```

#### 3. è©•åˆ†ç®—æ³•å®¢è£½åŒ–

```python
class CustomScoringAlgorithm:
    """è‡ªå®šç¾©è©•åˆ†ç®—æ³•åŸºé¡"""
    
    def calculate_pronunciation_score(self, audio_features, text_content, difficulty):
        """
        è¨ˆç®—ç™¼éŸ³åˆ†æ•¸
        
        Args:
            audio_features: éŸ³é »ç‰¹å¾µæ•¸æ“š
            text_content: è­˜åˆ¥æ–‡æœ¬å…§å®¹  
            difficulty: é›£åº¦ç´šåˆ¥
            
        Returns:
            int: ç™¼éŸ³åˆ†æ•¸ (0-100)
        """
        base_score = self._analyze_audio_quality(audio_features)
        text_bonus = self._analyze_text_complexity(text_content)
        difficulty_adjustment = self._get_difficulty_modifier(difficulty)
        
        return min(100, max(0, base_score + text_bonus + difficulty_adjustment))
    
    def _analyze_audio_quality(self, features):
        """åˆ†æéŸ³é »å“è³ª"""
        # å¯¦ç¾éŸ³é »å“è³ªåˆ†æé‚è¼¯
        pass
    
    def _analyze_text_complexity(self, text):
        """åˆ†ææ–‡æœ¬è¤‡é›œåº¦"""
        # å¯¦ç¾æ–‡æœ¬è¤‡é›œåº¦åˆ†æ
        pass
```

#### 4. UIçµ„ä»¶æ“´å±•

```python
# åœ¨ app.py ä¸­æ–°å¢è‡ªå®šç¾©çµ„ä»¶
def create_advanced_feedback_panel():
    """å‰µå»ºé€²éšå›é¥‹é¢æ¿"""
    with gr.Column(elem_classes="advanced-feedback-panel") as panel:
        # ç™¼éŸ³ç†±åŠ›åœ–
        pronunciation_heatmap = gr.Plot(
            label="ğŸ”¥ ç™¼éŸ³ç†±åŠ›åœ–",
            show_label=True
        )
        
        # èªèª¿æ›²ç·šåœ–
        intonation_curve = gr.Plot(
            label="ğŸ“ˆ èªèª¿åˆ†ææ›²ç·š", 
            show_label=True
        )
        
        # å°æ¯”åˆ†æ
        comparison_chart = gr.BarPlot(
            label="ğŸ“Š èˆ‡æ¨™æº–ç™¼éŸ³å°æ¯”",
            x="ç™¼éŸ³è¦ç´ ",
            y="ç›¸ä¼¼åº¦åˆ†æ•¸"
        )
    
    return panel, pronunciation_heatmap, intonation_curve, comparison_chart
```

### ğŸ§ª æ¸¬è©¦æ¡†æ¶

#### 1. å–®å…ƒæ¸¬è©¦

```python
# tests/test_models.py
import unittest
from models import ModelManager

class TestModelManager(unittest.TestCase):
    def setUp(self):
        self.model_manager = ModelManager(gpu_memory_limit=8)
    
    def test_device_detection(self):
        """æ¸¬è©¦è¨­å‚™æª¢æ¸¬åŠŸèƒ½"""
        device_info = self.model_manager.get_device_info()
        self.assertIn('device', device_info)
        self.assertIn('use_gpu', device_info)
    
    def test_whisper_transcription(self):
        """æ¸¬è©¦WhisperèªéŸ³è­˜åˆ¥"""
        # ä½¿ç”¨æ¸¬è©¦éŸ³é »æ–‡ä»¶
        result = self.model_manager.transcribe_audio("test_audio.wav")
        self.assertIsNotNone(result)
        self.assertIsInstance(result, str)
    
    def tearDown(self):
        self.model_manager.clear_gpu_memory()

# é‹è¡Œæ¸¬è©¦
python -m pytest tests/ -v
```

#### 2. é›†æˆæ¸¬è©¦

```python
# tests/test_integration.py
import gradio as gr
from app import demo

def test_gradio_interface():
    """æ¸¬è©¦Gradioç•Œé¢é›†æˆ"""
    # æ¸¬è©¦ç•Œé¢å•Ÿå‹•
    assert demo is not None
    
    # æ¸¬è©¦ä¸»è¦åŠŸèƒ½
    inputs = ["test_audio.wav", "è‹±æ–‡", "ä¸­ç´š"]
    outputs = demo.process(inputs)
    assert len(outputs) > 0

def test_end_to_end_workflow():
    """ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹æ¸¬è©¦"""
    from processors import get_conversation_manager
    
    manager = get_conversation_manager()
    result = manager.process_user_input(
        "test_audio.wav",
        "æ©Ÿå ´å°è©± (Airport Conversation)",
        "",
        "ä¸­ç´š (TOEIC 605-780åˆ†)"
    )
    
    assert result["success"] == True
    assert "recognized_text" in result
    assert "pronunciation_score" in result
```

#### 3. æ€§èƒ½æ¸¬è©¦

```python
# tests/test_performance.py
import time
import memory_profiler

@memory_profiler.profile
def test_memory_usage():
    """æ¸¬è©¦è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    from models import get_model_manager
    
    manager = get_model_manager()
    
    # é€£çºŒè™•ç†æ¸¬è©¦
    for i in range(10):
        result = manager.transcribe_audio(f"test_audio_{i}.wav")
        assert result is not None

def test_response_time():
    """æ¸¬è©¦å›æ‡‰æ™‚é–“"""
    from processors import get_conversation_manager
    
    manager = get_conversation_manager()
    
    start_time = time.time()
    result = manager.process_user_input(
        "test_audio.wav", 
        "æ—¥å¸¸ç¤¾äº¤",
        "",
        "ä¸­ç´š"
    )
    end_time = time.time()
    
    response_time = end_time - start_time
    assert response_time < 10.0  # æ‡‰åœ¨10ç§’å…§å®Œæˆ
    print(f"å›æ‡‰æ™‚é–“: {response_time:.2f}ç§’")
```

### ğŸ“¦ éƒ¨ç½²æŒ‡å—

#### 1. Dockerå®¹å™¨åŒ–éƒ¨ç½²

```dockerfile
# Dockerfile
FROM pytorch/pytorch:2.1.0-cuda11.8-runtime-ubuntu20.04

WORKDIR /app

# å®‰è£ç³»çµ±ä¾è³´
RUN apt-get update && apt-get install -y \
    ffmpeg \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# è¤‡è£½éœ€æ±‚æ–‡ä»¶
COPY requirements.txt .

# å®‰è£Pythonä¾è³´
RUN pip install --no-cache-dir -r requirements.txt

# è¤‡è£½æ‡‰ç”¨ç¨‹å¼ç¢¼
COPY . .

# é ä¸‹è¼‰æ¨¡å‹ (å¯é¸)
RUN python -c "import whisper; whisper.load_model('medium')"

# æš´éœ²ç«¯å£
EXPOSE 7861

# å•Ÿå‹•æŒ‡ä»¤
CMD ["python", "app.py"]
```

```bash
# æ§‹å»ºå’Œé‹è¡Œå®¹å™¨
docker build -t language-assistant .
docker run -p 7861:7861 --gpus all language-assistant
```

#### 2. é›²ç«¯éƒ¨ç½²é…ç½®

```yaml
# docker-compose.yml
version: '3.8'
services:
  language-assistant:
    build: .
    ports:
      - "7861:7861"
    environment:
      - GPU_MEMORY_LIMIT=16
      - GRADIO_SHARE=true
    volumes:
      - ./models_cache:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

#### 3. ç”Ÿç”¢ç’°å¢ƒå„ªåŒ–

```python
# production_config.py
PRODUCTION_CONFIG = {
    "model_settings": {
        "whisper_model": "medium",      # å¹³è¡¡æ€§èƒ½èˆ‡æº–ç¢ºåº¦
        "gpu_memory_limit": 16,         # ä¿ç•™è¨˜æ†¶é«”ç·©è¡
        "enable_model_caching": True,   # å•Ÿç”¨æ¨¡å‹ç·©å­˜
        "batch_processing": False       # é—œé–‰æ‰¹æ¬¡è™•ç†
    },
    
    "security_settings": {
        "enable_https": True,           # å•Ÿç”¨HTTPS
        "cors_origins": ["*"],          # CORSè¨­å®š
        "rate_limiting": {              # æµé‡é™åˆ¶
            "requests_per_minute": 60,
            "requests_per_hour": 1000
        }
    },
    
    "monitoring": {
        "enable_metrics": True,         # å•Ÿç”¨ç›£æ§æŒ‡æ¨™
        "log_level": "INFO",           # æ—¥èªŒç´šåˆ¥
        "health_check_endpoint": "/health"  # å¥åº·æª¢æŸ¥
    }
}
```

## ğŸ“„ æŠ€è¡“æ–‡æª”

### ğŸ”¬ æ ¸å¿ƒæ¼”ç®—æ³•èªªæ˜

#### 1. èªéŸ³è­˜åˆ¥æµç¨‹

```mermaid
sequenceDiagram
    participant U as ç”¨æˆ¶
    participant W as Whisper Model
    participant P as Processor
    participant Q as Qwen2-Audio
    
    U->>W: éŸ³é »è¼¸å…¥
    W->>W: èªéŸ³è½‰æ–‡å­—è™•ç†
    W->>P: è¿”å›è­˜åˆ¥æ–‡æœ¬
    P->>P: æ–‡æœ¬é è™•ç†èˆ‡åˆ†æ
    P->>Q: éŸ³é »+æ–‡æœ¬å¤šæ¨¡æ…‹åˆ†æ
    Q->>P: è©³ç´°èªè¨€å­¸åˆ†æçµæœ
    P->>U: æœ€çµ‚è©•ä¼°å ±å‘Š
```

#### 2. é›£åº¦èª¿æ•´ç®—æ³•

```python
def calculate_difficulty_adjusted_score(base_score, difficulty_level, user_progress):
    """
    åŸºæ–¼é›£åº¦å’Œç”¨æˆ¶é€²åº¦çš„å‹•æ…‹è©•åˆ†ç®—æ³•
    
    Formula:
    adjusted_score = base_score + difficulty_modifier + progress_bonus + encouragement_factor
    """
    
    # é›£åº¦èª¿æ•´ä¿‚æ•¸
    difficulty_modifiers = {
        "åˆå­¸è€…": +15,    # æ›´é¼“å‹µçš„è©•åˆ†
        "åˆç´š": +10,      # é©åº¦é¼“å‹µ
        "ä¸­ç´š": 0,        # æ¨™æº–è©•åˆ†
        "ä¸­é«˜ç´š": -5,     # æ›´åš´æ ¼æ¨™æº–
        "é«˜ç´š": -10       # å°ˆæ¥­ç´šæ¨™æº–
    }
    
    # é€²æ­¥çå‹µä¿‚æ•¸
    progress_bonus = min(10, user_progress.improvement_rate * 5)
    
    # é¼“å‹µå› å­ (é¿å…é€£çºŒä½åˆ†æ‰“æ“Šä¿¡å¿ƒ)
    encouragement_factor = 0
    if user_progress.recent_low_scores >= 3:
        encouragement_factor = 8
    
    return min(100, max(40, 
        base_score + 
        difficulty_modifiers.get(difficulty_level, 0) + 
        progress_bonus + 
        encouragement_factor
    ))
```

#### 3. è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥

```python
class MemoryManagementStrategy:
    """
    ä¸‰å±¤è¨˜æ†¶é«”ç®¡ç†ç­–ç•¥
    """
    
    def __init__(self):
        self.strategy_levels = {
            "normal": self._normal_operation,      # æ­£å¸¸é‹è¡Œ
            "conservative": self._conservative_mode,  # ä¿å®ˆæ¨¡å¼
            "emergency": self._emergency_mode      # ç·Šæ€¥æ¨¡å¼
        }
    
    def _normal_operation(self):
        """æ­£å¸¸é‹è¡Œæ¨¡å¼ (< 70% è¨˜æ†¶é«”ä½¿ç”¨)"""
        return {
            "model_precision": "float16",
            "batch_size": 1,
            "cache_enabled": True,
            "background_cleanup": False
        }
    
    def _conservative_mode(self):
        """ä¿å®ˆæ¨¡å¼ (70-85% è¨˜æ†¶é«”ä½¿ç”¨)"""
        return {
            "model_precision": "float16",
            "batch_size": 1,
            "cache_enabled": False,
            "background_cleanup": True,
            "frequent_gc": True
        }
    
    def _emergency_mode(self):
        """ç·Šæ€¥æ¨¡å¼ (> 85% è¨˜æ†¶é«”ä½¿ç”¨)"""
        return {
            "model_precision": "float32",
            "unload_secondary_models": True,
            "force_cpu_mode": True,
            "immediate_cleanup": True
        }
```

### ğŸ“Š APIæ–‡æª”

#### 1. æ ¸å¿ƒAPIæ¥å£

```python
class LanguageAssistantAPI:
    """èªè¨€åŠ©æ•™æ ¸å¿ƒAPI"""
    
    def transcribe_speech(self, audio_file: str, language: str = "en") -> dict:
        """
        èªéŸ³è­˜åˆ¥API
        
        Args:
            audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
            language: èªè¨€ä»£ç¢¼ (en, zh, ja, ko)
            
        Returns:
            {
                "text": "è­˜åˆ¥çš„æ–‡å­—å…§å®¹",
                "confidence": 0.95,
                "duration": 5.2,
                "language": "en"
            }
        """
        pass
    
    def analyze_pronunciation(self, 
                            audio_file: str, 
                            text: str,
                            difficulty: str,
                            scenario: str) -> dict:
        """
        ç™¼éŸ³åˆ†æAPI
        
        Args:
            audio_file: éŸ³é »æ–‡ä»¶è·¯å¾‘
            text: å°æ‡‰çš„æ–‡å­—å…§å®¹
            difficulty: é›£åº¦ç´šåˆ¥
            scenario: å°è©±å ´æ™¯
            
        Returns:
            {
                "pronunciation_score": 85,
                "fluency_score": 78,
                "detailed_analysis": "...",
                "suggestions": ["..."],
                "next_response": "..."
            }
        """
        pass
    
    def get_system_status(self) -> dict:
        """
        ç³»çµ±ç‹€æ…‹API
        
        Returns:
            {
                "gpu_available": true,
                "memory_usage": {
                    "gpu": "8.5GB / 24GB",
                    "cpu": "12GB / 32GB"
                },
                "models_loaded": {
                    "whisper": true,
                    "qwen2_audio": true
                },
                "performance_metrics": {...}
            }
        """
        pass
```

#### 2. WebSocketå³æ™‚é€šè¨Š

```python
import asyncio
import websockets

class RealTimeLanguageAssistant:
    """å³æ™‚èªè¨€åŠ©æ•™WebSocketæœå‹™"""
    
    async def handle_client(self, websocket, path):
        """è™•ç†å®¢æˆ¶ç«¯é€£ç·š"""
        try:
            async for message in websocket:
                data = json.loads(message)
                
                if data["type"] == "audio_stream":
                    # è™•ç†å³æ™‚éŸ³é »æµ
                    result = await self.process_audio_stream(data["audio"])
                    await websocket.send(json.dumps(result))
                
                elif data["type"] == "text_input":
                    # è™•ç†æ–‡å­—è¼¸å…¥
                    response = await self.generate_response(data["text"])
                    await websocket.send(json.dumps(response))
                    
        except websockets.exceptions.ConnectionClosed:
            print("å®¢æˆ¶ç«¯é€£ç·šå·²æ–·é–‹")
    
    def start_server(self, host="localhost", port=8765):
        """å•Ÿå‹•WebSocketæœå‹™å™¨"""
        start_server = websockets.serve(self.handle_client, host, port)
        asyncio.get_event_loop().run_until_complete(start_server)
        asyncio.get_event_loop().run_forever()
```

### ğŸ”’ å®‰å…¨æ€§èˆ‡éš±ç§

#### 1. æ•¸æ“šä¿è­·æªæ–½

```python
class PrivacyProtection:
    """éš±ç§ä¿è­·æ©Ÿåˆ¶"""
    
    def __init__(self):
        self.encryption_key = self._generate_encryption_key()
    
    def encrypt_audio_data(self, audio_data: bytes) -> bytes:
        """åŠ å¯†éŸ³é »æ•¸æ“š"""
        from cryptography.fernet import Fernet
        fernet = Fernet(self.encryption_key)
        return fernet.encrypt(audio_data)
    
    def anonymize_user_data(self, user_data: dict) -> dict:
        """åŒ¿ååŒ–ç”¨æˆ¶æ•¸æ“š"""
        anonymized = user_data.copy()
        
        # ç§»é™¤å€‹äººè­˜åˆ¥ä¿¡æ¯
        anonymized.pop('user_id', None)
        anonymized.pop('ip_address', None)
        anonymized.pop('device_info', None)
        
        # éŸ³é »æ•¸æ“šä¸æŒä¹…åŒ–å­˜å„²
        if 'audio_file' in anonymized:
            anonymized['audio_file'] = None
        
        return anonymized
    
    def auto_cleanup_temp_files(self):
        """è‡ªå‹•æ¸…ç†è‡¨æ™‚æ–‡ä»¶"""
        import os
        import time
        
        temp_dirs = ['temp_audio', 'user_recordings']
        for temp_dir in temp_dirs:
            if os.path.exists(temp_dir):
                for file in os.listdir(temp_dir):
                    file_path = os.path.join(temp_dir, file)
                    # åˆªé™¤è¶…é1å°æ™‚çš„è‡¨æ™‚æ–‡ä»¶
                    if time.time() - os.path.getctime(file_path) > 3600:
                        os.remove(file_path)
```

#### 2. è¨ªå•æ§åˆ¶

```python
from functools import wraps
import jwt
import datetime

def require_authentication(f):
    """APIèªè­‰è£é£¾å™¨"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        token = request.headers.get('Authorization')
        
        if not token:
            return jsonify({'error': 'ç¼ºå°‘èªè­‰ä»¤ç‰Œ'}), 401
        
        try:
            # é©—è­‰JWTä»¤ç‰Œ
            payload = jwt.decode(token, SECRET_KEY, algorithms=['HS256'])
            current_user = payload['user_id']
            
        except jwt.ExpiredSignatureError:
            return jsonify({'error': 'ä»¤ç‰Œå·²éæœŸ'}), 401
        except jwt.InvalidTokenError:
            return jsonify({'error': 'ç„¡æ•ˆä»¤ç‰Œ'}), 401
        
        return f(current_user, *args, **kwargs)
    
    return decorated_function

class RateLimiter:
    """APIæµé‡é™åˆ¶"""
    
    def __init__(self, max_requests=60, time_window=60):
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = {}
    
    def is_allowed(self, client_id: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦å…è¨±è«‹æ±‚"""
        now = datetime.datetime.now()
        
        if client_id not in self.requests:
            self.requests[client_id] = []
        
        # æ¸…ç†éæœŸè¨˜éŒ„
        self.requests[client_id] = [
            req_time for req_time in self.requests[client_id]
            if (now - req_time).seconds < self.time_window
        ]
        
        # æª¢æŸ¥æ˜¯å¦è¶…éé™åˆ¶
        if len(self.requests[client_id]) >= self.max_requests:
            return False
        
        self.requests[client_id].append(now)
        return True
```

## ğŸ™ è‡´è¬

### ğŸ›ï¸ å­¸è¡“æ©Ÿæ§‹
- **æ¸…è¯å¤§å­¸é›»æ©Ÿå·¥ç¨‹å­¸ç³»** - æä¾›å„ªç§€çš„å­¸è¡“ç’°å¢ƒèˆ‡ç ”ç©¶è³‡æº
- **æ¸…è¯å¤§å­¸è¨ˆç®—æ©Ÿèˆ‡é€šä¿¡ä¸­å¿ƒ** - æä¾›GPUé‹ç®—è³‡æºæ”¯æŒ

### ğŸ¤– é–‹æºç¤¾ç¾¤èˆ‡æŠ€è¡“å¤¥ä¼´

#### AIæ¨¡å‹æä¾›è€…
- **OpenAI** - WhisperèªéŸ³è­˜åˆ¥æ¨¡å‹çš„é–‹ç™¼èˆ‡é–‹æº
- **é˜¿é‡Œå·´å·´é”æ‘©é™¢** - Qwen2-Audioå¤šæ¨¡æ…‹èªè¨€æ¨¡å‹
- **Hugging Face** - æ¨¡å‹æ‰˜ç®¡å¹³å°èˆ‡Transformersæ¡†æ¶

#### é–‹æºæ¡†æ¶è²¢ç»è€…  
- **PyTorchåœ˜éšŠ** - æ·±åº¦å­¸ç¿’æ¡†æ¶åŸºç¤
- **Gradioé–‹ç™¼åœ˜éšŠ** - æ©Ÿå™¨å­¸ç¿’æ‡‰ç”¨ç•Œé¢æ¡†æ¶
- **Pythonç¤¾ç¾¤** - è±å¯Œçš„ç”Ÿæ…‹ç³»çµ±èˆ‡å¥—ä»¶æ”¯æŒ

### ğŸ”¬ ç ”ç©¶åƒè€ƒ

#### å­¸è¡“è«–æ–‡
1. Radford, A., et al. (2023). "Robust Speech Recognition via Large-Scale Weak Supervision" - Whisperæ¨¡å‹åŸç†
2. Chu, X., et al. (2023). "Qwen2-Audio: Advancing Universal Audio Understanding" - Qwen2-AudioæŠ€è¡“ç´°ç¯€
3. Wang, C., et al. (2022). "Speech Recognition Error Detection and Correction" - èªéŸ³è­˜åˆ¥å„ªåŒ–æŠ€è¡“

#### æŠ€è¡“æ–‡æª”èˆ‡è³‡æº
- [Whisper Official Documentation](https://openai.com/research/whisper)
- [Qwen2-Audio Model Card](https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct)
- [PyTorch CUDA Best Practices](https://pytorch.org/docs/stable/notes/cuda.html)
- [Gradio Documentation](https://gradio.app/docs/)

### ğŸ‘¥ ç‰¹åˆ¥æ„Ÿè¬

#### æŒ‡å°èˆ‡æ”¯æŒ
- **æŒ‡å°æ•™æˆ** - å­¸è¡“æ–¹å‘æŒ‡å°èˆ‡å°ˆæ¥­å»ºè­°
- **åŒå­¸å€‘** - æ¸¬è©¦å›é¥‹èˆ‡åŠŸèƒ½å»ºè­°
- **é–‹æºç¤¾ç¾¤** - æŠ€è¡“å•é¡Œè§£ç­”èˆ‡ç¶“é©—åˆ†äº«

#### æ¸¬è©¦è²¢ç»è€…
æ„Ÿè¬æ‰€æœ‰åƒèˆ‡ç³»çµ±æ¸¬è©¦çš„ä½¿ç”¨è€…ï¼Œä½ å€‘çš„å›é¥‹å°æ–¼æ”¹å–„ç³»çµ±åŠŸèƒ½è‡³é—œé‡è¦ï¼š
- ç™¼éŸ³è©•ä¼°æº–ç¢ºåº¦æ¸¬è©¦
- å¤šå ´æ™¯å°è©±å“è³ªè©•ä¼°  
- ç”¨æˆ¶é«”é©—å„ªåŒ–å»ºè­°
- ç³»çµ±ç©©å®šæ€§å£“åŠ›æ¸¬è©¦

---

## ğŸ“ è¯çµ¡è³‡è¨Š

### ğŸ‘¨â€ğŸ“ å°ˆæ¡ˆä½œè€…
- **å§“å**: æ¥Šå‚‘ç¿”
- **å­¸è™Ÿ**: 113061529  
- **ç³»æ‰€**: æ¸…è¯å¤§å­¸é›»æ©Ÿå·¥ç¨‹å­¸ç³»ç¢©å£«ç­
- **å­¸å¹´**: 113å­¸å¹´åº¦

### ğŸ“§ è¯çµ¡æ–¹å¼
- **å­¸æ ¡ä¿¡ç®±**: [s113061529@m113.nthu.edu.tw](mailto:s113061529@m113.nthu.edu.tw)
- **å°ˆæ¡ˆGitHub**: [å¾…è£œå……]
- **æŠ€è¡“éƒ¨è½æ ¼**: [å¾…è£œå……]

### ğŸ› å•é¡Œå›å ±
å¦‚æœæ‚¨åœ¨ä½¿ç”¨éç¨‹ä¸­é‡åˆ°ä»»ä½•å•é¡Œï¼Œæ­¡è¿é€éä»¥ä¸‹æ–¹å¼å›å ±ï¼š

1. **GitHub Issues**: åœ¨å°ˆæ¡ˆrepositoryä¸­é–‹å•Ÿissue
2. **é›»å­éƒµä»¶**: ç›´æ¥ç™¼é€è©³ç´°å•é¡Œæè¿°è‡³è¯çµ¡ä¿¡ç®±
3. **æŠ€è¡“è¨è«–**: æ­¡è¿å°±æŠ€è¡“å¯¦ç¾ç´°ç¯€é€²è¡Œå­¸è¡“è¨è«–

### ğŸ¤ åˆä½œæ©Ÿæœƒ
æ­¡è¿å°ä»¥ä¸‹é ˜åŸŸæœ‰èˆˆè¶£çš„ç ”ç©¶è€…æˆ–é–‹ç™¼è€…è¯çµ¡ï¼š
- å¤šæ¨¡æ…‹AIæ‡‰ç”¨ç ”ç©¶
- èªè¨€å­¸ç¿’æŠ€è¡“é–‹ç™¼
- èªéŸ³è™•ç†ç®—æ³•å„ªåŒ–
- æ•™è‚²ç§‘æŠ€å‰µæ–°æ‡‰ç”¨

---

<div align="center">

### ğŸ‰ æ„Ÿè¬ä½¿ç”¨æ™ºèƒ½èªè¨€å­¸ç¿’åŠ©æ•™ç³»çµ±ï¼

**è®“AIæˆç‚ºæ‚¨èªè¨€å­¸ç¿’è·¯ä¸Šçš„æœ€ä½³å¤¥ä¼´** ğŸš€

[![æ¸…è¯å¤§å­¸](https://img.shields.io/badge/æ¸…è¯å¤§å­¸-é›»æ©Ÿå·¥ç¨‹å­¸ç³»-purple.svg)](https://web.ee.nthu.edu.tw/)
[![Python](https://img.shields.io/badge/Made%20with-Python-blue.svg)](https://python.org)
[![AI](https://img.shields.io/badge/Powered%20by-AI-green.svg)](https://openai.com)
[![Love](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg)](https://github.com)

**Happy Learning! ğŸ“šâœ¨**

</div>