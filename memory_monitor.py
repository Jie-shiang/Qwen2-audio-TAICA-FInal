# -*- coding: utf-8 -*-
"""
memory_monitor.py - è¨˜æ†¶é«”ç›£æ§æ¨¡çµ„
ç›£æ§GPUè¨˜æ†¶é«”ä½¿ç”¨é‡ï¼Œè¶…éé™åˆ¶æ™‚è‡ªå‹•æš«åœCode
"""

import torch
import psutil
import threading
import time
import os
import signal
import gc
import warnings

class MemoryMonitor:
    
    def __init__(self, gpu_limit_gb=20, cpu_limit_gb=32, check_interval=5):
        """        
        Args:
            gpu_limit_gb (int): GPUè¨˜æ†¶é«”é™åˆ¶ï¼ˆGBï¼‰
            cpu_limit_gb (int): CPUè¨˜æ†¶é«”é™åˆ¶ï¼ˆGBï¼‰
            check_interval (int): æª¢æŸ¥é–“éš”ï¼ˆç§’ï¼‰
        """
        self.gpu_limit_gb = gpu_limit_gb
        self.cpu_limit_gb = cpu_limit_gb
        self.check_interval = check_interval
        self.monitoring = False
        self.monitor_thread = None
        self.emergency_cleanup_triggered = False
        
        # æª¢æŸ¥CUDAå¯ç”¨æ€§
        self.cuda_available = torch.cuda.is_available()
        if self.cuda_available:
            self.gpu_count = torch.cuda.device_count()
            print(f"ğŸ–¥ï¸  æª¢æ¸¬åˆ° {self.gpu_count} å€‹GPUï¼Œè¨˜æ†¶é«”é™åˆ¶: {gpu_limit_gb}GB")
        else:
            print("âš ï¸  æœªæª¢æ¸¬åˆ°CUDAï¼Œåƒ…ç›£æ§CPUè¨˜æ†¶é«”")
    
    def get_gpu_memory_usage(self):
        if not self.cuda_available:
            return {}
        
        gpu_memory = {}
        for i in range(self.gpu_count):
            try:
                # ç²å–GPUè¨˜æ†¶é«”ä¿¡æ¯
                memory_allocated = torch.cuda.memory_allocated(i) / 1024**3  # GB
                memory_reserved = torch.cuda.memory_reserved(i) / 1024**3   # GB
                memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3  # GB
                
                gpu_memory[f"GPU_{i}"] = {
                    "allocated": memory_allocated,
                    "reserved": memory_reserved,
                    "total": memory_total,
                    "usage_percent": (memory_reserved / memory_total) * 100
                }
            except Exception as e:
                print(f"âš ï¸  ç²å–GPU {i} è¨˜æ†¶é«”ä¿¡æ¯å¤±æ•—: {e}")
        
        return gpu_memory
    
    def get_cpu_memory_usage(self):
        try:
            # ç²å–ç³»çµ±è¨˜æ†¶é«”ä¿¡æ¯
            memory = psutil.virtual_memory()
            process = psutil.Process(os.getpid())
            process_memory = process.memory_info().rss / 1024**3  # GB
            
            return {
                "total": memory.total / 1024**3,
                "available": memory.available / 1024**3,
                "used": memory.used / 1024**3,
                "percent": memory.percent,
                "process_usage": process_memory
            }
        except Exception as e:
            print(f"âš ï¸  ç²å–CPUè¨˜æ†¶é«”ä¿¡æ¯å¤±æ•—: {e}")
            return {}
    
    def emergency_cleanup(self):
        if self.emergency_cleanup_triggered:
            return
        
        self.emergency_cleanup_triggered = True
        print("ğŸš¨ é–‹å§‹ç·Šæ€¥è¨˜æ†¶é«”æ¸…ç†...")
        
        try:
            # æ¸…ç†GPUè¨˜æ†¶é«”
            if self.cuda_available:
                print("ğŸ§¹ æ¸…ç†GPUè¨˜æ†¶é«”...")
                for i in range(self.gpu_count):
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            
            # å¼·åˆ¶åƒåœ¾æ”¶é›†
            print("ğŸ§¹ åŸ·è¡Œåƒåœ¾æ”¶é›†...")
            gc.collect()
            
            # ç­‰å¾…ä¸€æ®µæ™‚é–“è®“æ¸…ç†ç”Ÿæ•ˆ
            time.sleep(2)
            
            # å†æ¬¡æª¢æŸ¥è¨˜æ†¶é«”ä½¿ç”¨
            gpu_memory = self.get_gpu_memory_usage()
            for gpu_id, info in gpu_memory.items():
                if info["reserved"] > self.gpu_limit_gb:
                    print(f"âš ï¸  {gpu_id} è¨˜æ†¶é«”ä»è¶…éé™åˆ¶: {info['reserved']:.2f}GB")
                    return False
            
            print("âœ… ç·Šæ€¥æ¸…ç†å®Œæˆ")
            self.emergency_cleanup_triggered = False
            return True
            
        except Exception as e:
            print(f"âŒ ç·Šæ€¥æ¸…ç†å¤±æ•—: {e}")
            return False
    
    def force_kill_program(self, reason):
        print(f"\n{'='*50}")
        print(f"ğŸš¨ è¨˜æ†¶é«”ä½¿ç”¨è¶…éé™åˆ¶ï¼")
        print(f"ğŸ“Š åŸå› : {reason}")
        print(f"ğŸ”„ å˜—è©¦ç·Šæ€¥æ¸…ç†...")
        
        # å˜—è©¦ç·Šæ€¥æ¸…ç†
        cleanup_success = self.emergency_cleanup()
        
        if not cleanup_success:
            print(f"âŒ ç·Šæ€¥æ¸…ç†å¤±æ•—ï¼Œå¼·åˆ¶çµ‚æ­¢ç¨‹åº...")
            print(f"â° ç¨‹åºå°‡åœ¨3ç§’å¾Œçµ‚æ­¢...")
            
            for i in range(3, 0, -1):
                print(f"â³ {i}...")
                time.sleep(1)
            
            print("ğŸ’€ ç¨‹åºå·²çµ‚æ­¢")
            os.kill(os.getpid(), signal.SIGTERM)
        else:
            print("âœ… ç·Šæ€¥æ¸…ç†æˆåŠŸï¼Œç¹¼çºŒé‹è¡Œ")
    
    def check_memory_usage(self):
        if self.cuda_available:
            gpu_memory = self.get_gpu_memory_usage()
            for gpu_id, info in gpu_memory.items():
                if info["reserved"] > self.gpu_limit_gb:
                    reason = f"{gpu_id} è¨˜æ†¶é«”ä½¿ç”¨: {info['reserved']:.2f}GB > {self.gpu_limit_gb}GB"
                    self.force_kill_program(reason)
                    return False
        
        cpu_memory = self.get_cpu_memory_usage()
        if cpu_memory and cpu_memory.get("process_usage", 0) > self.cpu_limit_gb:
            reason = f"CPUè¨˜æ†¶é«”ä½¿ç”¨: {cpu_memory['process_usage']:.2f}GB > {self.cpu_limit_gb}GB"
            self.force_kill_program(reason)
            return False
        
        return True
    
    def print_memory_status(self):
        print(f"\nğŸ“Š ==== è¨˜æ†¶é«”ä½¿ç”¨ç‹€æ³ ====")
        
        if self.cuda_available:
            gpu_memory = self.get_gpu_memory_usage()
            for gpu_id, info in gpu_memory.items():
                status = "ğŸ”´" if info["reserved"] > self.gpu_limit_gb * 0.8 else "ğŸŸ¢"
                print(f"{status} {gpu_id}: {info['reserved']:.2f}GB / {info['total']:.2f}GB ({info['usage_percent']:.1f}%)")
        
        cpu_memory = self.get_cpu_memory_usage()
        if cpu_memory:
            status = "ğŸ”´" if cpu_memory["process_usage"] > self.cpu_limit_gb * 0.8 else "ğŸŸ¢"
            print(f"{status} CPUé€²ç¨‹: {cpu_memory['process_usage']:.2f}GB")
            print(f"ğŸ–¥ï¸  ç³»çµ±è¨˜æ†¶é«”: {cpu_memory['used']:.2f}GB / {cpu_memory['total']:.2f}GB ({cpu_memory['percent']:.1f}%)")
        
        print(f"{'='*30}")
    
    def monitor_loop(self):
        print(f"ğŸ” è¨˜æ†¶é«”ç›£æ§å·²å•Ÿå‹• (GPUé™åˆ¶: {self.gpu_limit_gb}GB, æª¢æŸ¥é–“éš”: {self.check_interval}ç§’)")
        
        while self.monitoring:
            try:
                if not self.check_memory_usage():
                    break
                
                if int(time.time()) % 30 == 0:
                    self.print_memory_status()
                
                time.sleep(self.check_interval)
                
            except Exception as e:
                print(f"âš ï¸  ç›£æ§å¾ªç’°å‡ºéŒ¯: {e}")
                time.sleep(self.check_interval)
    
    def start_monitoring(self):
        if self.monitoring:
            print("âš ï¸  ç›£æ§å·²ç¶“åœ¨é‹è¡Œä¸­")
            return
        
        self.monitoring = True
        self.monitor_thread = threading.Thread(target=self.monitor_loop, daemon=True)
        self.monitor_thread.start()
        print("âœ… è¨˜æ†¶é«”ç›£æ§å·²å•Ÿå‹•")
    
    def stop_monitoring(self):
        if not self.monitoring:
            print("âš ï¸  ç›£æ§æœªåœ¨é‹è¡Œ")
            return
        
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=5)
        print("ğŸ›‘ è¨˜æ†¶é«”ç›£æ§å·²åœæ­¢")
    
    def get_current_status(self):
        status = {
            "gpu_memory": self.get_gpu_memory_usage(),
            "cpu_memory": self.get_cpu_memory_usage(),
            "limits": {
                "gpu_limit_gb": self.gpu_limit_gb,
                "cpu_limit_gb": self.cpu_limit_gb
            },
            "monitoring": self.monitoring
        }
        return status

memory_monitor = None

def get_memory_monitor(gpu_limit_gb=20, cpu_limit_gb=32, check_interval=5):
    global memory_monitor
    if memory_monitor is None:
        memory_monitor = MemoryMonitor(gpu_limit_gb, cpu_limit_gb, check_interval)
    return memory_monitor

def start_memory_monitoring(gpu_limit_gb=20, cpu_limit_gb=32, check_interval=5):
    monitor = get_memory_monitor(gpu_limit_gb, cpu_limit_gb, check_interval)
    monitor.start_monitoring()
    return monitor

def stop_memory_monitoring():
    global memory_monitor
    if memory_monitor:
        memory_monitor.stop_monitoring()

if __name__ == "__main__":
    print("æ¸¬è©¦è¨˜æ†¶é«”ç›£æ§å™¨...")
    monitor = start_memory_monitoring(gpu_limit_gb=20, check_interval=2)
    
    try:
        time.sleep(10)
        
        status = monitor.get_current_status()
        print("ç•¶å‰ç‹€æ…‹:", status)
        
    except KeyboardInterrupt:
        print("ç”¨æˆ¶ä¸­æ–·")
    finally:
        stop_memory_monitoring()